{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5a020e-dfce-4ab4-b503-bc0f74154576",
   "metadata": {},
   "source": [
    "# Python is used to run the tasks in a data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eb0731-692f-46f5-aafa-485fe3d25b7b",
   "metadata": {},
   "source": [
    "# [SETUP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b5f60b-2da0-41d9-be58-f410b365c194",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../setup.py --db_file tpch.db --sqlite_db_file example.db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8ff760-d6eb-4752-8ed6-5bfe81101aee",
   "metadata": {},
   "source": [
    "# Use the right data structure for your data access needs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc19517-aa21-46fb-b939-02fa06cdedd8",
   "metadata": {},
   "source": [
    "- most code logic that controls data pipeline will involve one for of control flow\n",
    "* if..elif..else\n",
    "* for loop\n",
    "\n",
    "You will mostly do ne of the following:\n",
    "1. iterate: operate on one element from a list of elements at a time\n",
    "2. lookup: quickly access value given a key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350f0248-873b-4d71-99c7-ee5a28429838",
   "metadata": {},
   "source": [
    "### `List` for iteration and `dict` for lookup\n",
    "\n",
    "1. `Lists`: In Python, lists are ideal for storing a collection of items that you want to iterate over. Lists are ordered, mutable, and can contain duplicate elements.\n",
    "2. `Dictionaries`: Dictionaries (dict) are perfect for situations where you need fast lookups by key. A dictionary stores key-value pairs and provides average O(1) time complexity for lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db92611f-1ecc-42aa-9169-5a5875c4d2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for Iteration\n",
    "names = [\"Alice\", \"Bob\", \"Charlie\"]\n",
    "for name in names:\n",
    "    print(f\"Hello, {name}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da14799a-07bd-4363-abd9-8651ad25c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict for Lookup\n",
    "age_lookup = {\n",
    "    \"Alice\": 30,\n",
    "    \"Bob\": 25,\n",
    "    \"Charlie\": 35\n",
    "}\n",
    "print(f\"Alice's age is {age_lookup['Alice']}\")  # Fast lookup by key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa6b117-7a88-4853-a659-071c0db8f176",
   "metadata": {},
   "source": [
    "### Functions allow you to reuse blocks of code\n",
    "\n",
    "A function is a block of code that can be re-used as needed. This allows for us to have logic defined in one place, making it easy to maintain and use.\n",
    "\n",
    "Often referred to as DRY (dont repeat yourself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92004e7c-e95e-4440-90d4-1a730bf78296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt_three(input_list):\n",
    "    result = []\n",
    "    for elt in input_list:\n",
    "        if elt > 3:\n",
    "            result.append(elt)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b087daf9-bd22-4a35-bc60-efaac4ab1b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_three([1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132c6c72-6f86-4c93-8616-277b653f0e5c",
   "metadata": {},
   "source": [
    "### Define a blueprint with a `Class` and create `Objects` from it\n",
    "\n",
    "Think of a class as a blueprint and objects as things created based on that blueprint\n",
    "\n",
    "In data pipelines we generally create a base class to represent \"how\" your data pieline should work (not what transformation it does).\n",
    "When creating pipelines you'd generally inherit from the base class and make necessary changes. \n",
    "\n",
    "add: inheritance image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b9826-07bf-4a18-b9f5-b3c62d5285af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExtractor:\n",
    "\n",
    "    def __init__(self, extractor_id):\n",
    "        self.extractor_id = extractor_id\n",
    "\n",
    "    def get_connection(self):\n",
    "        print(f'Getting {self.extractor_id}s connection')\n",
    "        return\n",
    "        # Some logic\n",
    "\n",
    "    def close_connection(self):\n",
    "        print(f'Closing {self.extractor_id}s connection')\n",
    "        # Some logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978c665c-80b7-4a08-816f-b637b215c57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data_extractor = DataExtractor(\"csv\")\n",
    "csv_data_extractor.get_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32028ae3-4ac9-4ed6-940f-1e4736e3739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data_extractor = DataExtractor(\"json\")\n",
    "json_data_extractor.get_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d8a37b-68a6-4808-ac4a-054193b92c48",
   "metadata": {},
   "source": [
    "**Inheritance**\n",
    "\n",
    "- base class: define functions and its arguments\n",
    "- child class: implement function specific to its use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3649c6c-8ccf-4d02-8f2b-887337540aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from abc import ABC, abstractmethod # python module to define abstract interfaces\n",
    "\n",
    "# Abstract class with abstract methods\n",
    "class SocialETL(ABC):\n",
    "    @abstractmethod\n",
    "    def extract(self, id, num_records, client):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self, social_data):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self, social_data, db_conn):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def run(self, db_conn, client, id, num_records):\n",
    "        pass\n",
    "\n",
    "# Concrete implementation of the abstract Class\n",
    "class RedditETL(SocialETL):\n",
    "    def extract(self, id, num_records, client):\n",
    "        # code to extract reddit data\n",
    "        print(\"Reddit extract\")\n",
    "\n",
    "    def transform(self, social_data):\n",
    "        # code to transform reddit data\n",
    "        print(\"Reddit transform\")\n",
    "\n",
    "    def load(self, social_data, db_conn):\n",
    "        # code to load reddit data into the final table\n",
    "        print(\"Reddit load\")\n",
    "\n",
    "    def run(self, db_conn, client, id, num_records):\n",
    "        # code to run extract, transform and load\n",
    "        print(\"Reddit ETL run\")\n",
    "\n",
    "# Concrete implementation of the abstract Class\n",
    "class TwitterETL(SocialETL):\n",
    "    def extract(self, id, num_records, client):\n",
    "        # code to extract reddit data\n",
    "        print(\"Twitter extract\")\n",
    "\n",
    "    def transform(self, social_data):\n",
    "        # code to transform reddit data\n",
    "        print(\"Twitter transform\")\n",
    "\n",
    "    def load(self, social_data, db_conn):\n",
    "        # code to load reddit data into the final table\n",
    "        print(\"Twitter load\")\n",
    "\n",
    "    def run(self, db_conn, client, id, num_records):\n",
    "        # code to run extract, transform and load\n",
    "        print(\"Twitter ETL run\")\n",
    "\n",
    "# This \"factory\" will acccept an input and give you the appropriate object that you can use to perform ETL\n",
    "def etl_factory(source):\n",
    "    factory = {\n",
    "        'Reddit': RedditETL(),\n",
    "        'Twitter': TwitterETL()\n",
    "    }\n",
    "    if source in factory:\n",
    "        return factory[source]\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"source {source} is not supported. Please pass a valid source.\"\n",
    "        )\n",
    "\n",
    "# calling code\n",
    "source = 'Reddit'\n",
    "social_etl = etl_factory(source)\n",
    "social_etl.run(db_conn='fake_db_conn', id='fake_id', num_records=100, client='fake_client')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8191a16e-ad84-442f-98d0-f978bc69fa1e",
   "metadata": {},
   "source": [
    "# Python can push data to/pull data from any system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e717570-0a3a-4fd5-8a71-e4f692a30ae3",
   "metadata": {},
   "source": [
    "In data pipelines, Python is used as the glue to move data between systems. Python can orchestrate movement of data across sytems in one of 2 main ways\n",
    "\n",
    "1. PUll data into the Python process and push it to the destination. This approach requires that the data pulled into process meaning that the data size should be handle-able by the Python process\n",
    "2. Instruct other systems to move/transform data. In this approach Python acts as an orchestrator telling the other systems what to do. \n",
    "\n",
    "add: image and details Data is stored on disk and processed in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a1c5e-c50f-4d90-955c-1d58282280f0",
   "metadata": {},
   "source": [
    "## Interact with databases using their specific Python packages \n",
    "\n",
    "Python is a very popular language ad:tiobe link, and as such most database engines have python libraries specifically designed to interact with them.\n",
    "\n",
    "e.g. sqlite3, postgres, mysql, snowflake, redshift, duckdb\n",
    "\n",
    "In data pipelines, this is typically use to instruct your database engine to transform data \n",
    "* pull data and push it into a different systems (some database engines have native feature to push data to cloud storage, e.g. snowflake -> S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4832f32f-dfb2-459e-9d76-62596e3ee225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to an SQLite database (or create it)\n",
    "conn = sqlite3.connect('example.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query data (pull)\n",
    "cursor.execute('SELECT * FROM users') # We can run any SQL query here\n",
    "rows = cursor.fetchall()\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0973b5-e0e1-4109-bd9c-8d077a55c4c2",
   "metadata": {},
   "source": [
    "## Interact with API endpoint using the `requests` package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb6839-5d3c-4b44-9a0b-0cd6791d75b9",
   "metadata": {},
   "source": [
    "In data pipelines you may have to interact (usually pull data from) with APIs. \n",
    "\n",
    "While there are many libraries to interact with APIs the most popular one is `requests`\n",
    "\n",
    "When pulling data from an API, remember to \n",
    "* paginate: api data pulls can only send back a limited set of data at a time (due to bandwith constraints). So you will usually have to make multiple calls to the same API.\n",
    "* query params: Most apis enable you to ask it for specific segments of data, you can do this using query params. Do this with add: feature\n",
    "* rate limiting: The API is usually powered by >=1 servers, and if you repeated call the API multiple times without any breaks you may overwhelm the server (DOS attack). To prevent this and for performance most APIs have a limit on the number of API calls you can perform a minute. Control with add: feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992b6b16-faef-4f39-ac47-757f914e86ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Pull data (GET request)\n",
    "response = requests.get('https://jsonplaceholder.typicode.com/posts')\n",
    "data = response.json()\n",
    "\n",
    "# Print the first post\n",
    "print(f'Data pulled: {data[0]}')\n",
    "\n",
    "# Push data (POST request)\n",
    "new_post = {\n",
    "    'title': 'New Post',\n",
    "    'body': 'This is the content of the new post.',\n",
    "    'userId': 1\n",
    "}\n",
    "response = requests.post('https://jsonplaceholder.typicode.com/posts', json=new_post)\n",
    "print(f'Data posted: {response.json()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d056bc15-8ffd-4833-90bc-17f275baaa52",
   "metadata": {},
   "source": [
    "## Interact with files in your filesystem with Python's standard libraries\n",
    "\n",
    "Python can read write to files of most formats csv, xml, json, parquet, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9bcb86-2d7a-4d05-b487-148a90268166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Write CSV to a local file\n",
    "data = [[\"Name\", \"Age\"], [\"Alice\", 30], [\"Bob\", 25]]\n",
    "filename = \"sample.csv\"\n",
    "with open(filename, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109a0b9-dbd4-4e09-ad48-32e4a314c5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat ./sample.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0327cbfb-ab73-4fd8-9a91-c27285efa82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Delete the file if it exists\n",
    "if os.path.exists(filename):\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2903287e-320a-48f5-b3b2-98440367af94",
   "metadata": {},
   "source": [
    "# Run SQL queries using Python\n",
    "\n",
    "Python can be used to run SQL queries to transform/ load data. \n",
    "\n",
    "Without Python, you'd need a system (like dbt which is itself a python library) to run your SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d642fd-786f-4b8c-a568-4f663f90adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to an SQLite database (or create it)\n",
    "conn = sqlite3.connect('example.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query data (pull)\n",
    "cursor.execute('SELECT * FROM users') # We can run any SQL query here\n",
    "rows = cursor.fetchall()\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "cursor.execute('INSERT INTO users (name, age) VALUES (?, ?)', ('Chester', 9000))\n",
    "cursor.execute('INSERT INTO users (name, age) VALUES (?, ?)', ('Geppato', 50))\n",
    "# conn.commit() # Uncomment this line, else the insert will not be committed into your databsae\n",
    "\n",
    "# Query data (pull)\n",
    "cursor.execute('SELECT id, count(*) FROM users GROUP BY id ORDER BY id') # We can run any SQL query here\n",
    "rows = cursor.fetchall()\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cb03d5-e4a8-465f-883b-05e36115e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to an SQLite database (or create it)\n",
    "conn = sqlite3.connect('example.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query data (pull)\n",
    "cursor.execute('SELECT * FROM users') # We can run any SQL query here\n",
    "rows = cursor.fetchall()\n",
    "for row in rows:\n",
    "    print(row)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7de1b70-5665-4ac4-bacb-78a9d6f82ea1",
   "metadata": {},
   "source": [
    "# Dataframes provides a Pythonic way to transform data\n",
    "\n",
    "Python popularized dataframe based data transformations with Pandas, which was a result of dataframe popularity in R for data science.\n",
    "\n",
    "Dataframe allows us to load data into memory (Pandas) and perform transformations. Most operations that can be done in SQL can be done in Dataframe.\n",
    "\n",
    "Lates tech like Polars can handle data that is larger in size than your python process memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f193c1a7-7597-42be-9497-87926ef79a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "db_file_name = './tpch.db'\n",
    "conn = duckdb.connect(db_file_name)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Connect to DuckDB and load TPC-H tables into Pandas DataFrames\n",
    "customer_df = cursor.sql(\"SELECT * FROM customer\").df()\n",
    "orders_df = cursor.sql(\"SELECT * FROM orders\").df()\n",
    "lineitem_df = cursor.sql(\"SELECT * FROM lineitem\").df()\n",
    "nation_df = cursor.sql(\"SELECT * FROM nation\").df()\n",
    "region_df = cursor.sql(\"SELECT * FROM region\").df()\n",
    "supplier_df = cursor.sql(\"SELECT * FROM supplier\").df()\n",
    "part_df = cursor.sql(\"SELECT * FROM part\").df()\n",
    "partsupp_df = cursor.sql(\"SELECT * FROM partsupp\").df()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9080b3a-d11b-4cca-9754-a2c0b35dbe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a85008e-166e-4136-9611-ee9dc0fc7d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'customer_df' is the DataFrame containing the customer table data\n",
    "filtered_df = customer_df[customer_df[\"c_nationkey\"] == 20].head(10)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e32277-89d4-4242-8858-275a63c78e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df[\n",
    "    ((customer_df[\"c_nationkey\"] == 20) & (customer_df[\"c_acctbal\"] > 1000)) |\n",
    "    (customer_df[\"c_nationkey\"] == 11)\n",
    "].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a542eb-6632-4b82-bc8f-570eff7579b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join\n",
    "inner_join_df = orders_df.merge(\n",
    "    lineitem_df,\n",
    "    left_on=\"o_orderkey\",\n",
    "    right_on=\"l_orderkey\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "inner_join_df[\n",
    "    (inner_join_df[\"o_orderdate\"] >= inner_join_df[\"l_shipdate\"] - pd.Timedelta(days=5)) &\n",
    "    (inner_join_df[\"o_orderdate\"] <= inner_join_df[\"l_shipdate\"] + pd.Timedelta(days=5))\n",
    "].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778f560c-ab74-4e60-8ed3-e83d715cefef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join\n",
    "left_join_df = orders_df.merge(\n",
    "    lineitem_df,\n",
    "    left_on=\"o_orderkey\",\n",
    "    right_on=\"l_orderkey\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "left_join_df[\n",
    "    (left_join_df[\"o_orderdate\"] >= left_join_df[\"l_shipdate\"] - pd.Timedelta(days=5)) &\n",
    "    (left_join_df[\"o_orderdate\"] <= left_join_df[\"l_shipdate\"] + pd.Timedelta(days=5))\n",
    "].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8072912c-5cd9-46fd-a77b-743f370d05eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right join\n",
    "right_join_df = orders_df.merge(\n",
    "    lineitem_df,\n",
    "    left_on=\"o_orderkey\",\n",
    "    right_on=\"l_orderkey\",\n",
    "    how=\"right\"\n",
    ")\n",
    "\n",
    "right_join_df[\n",
    "    (right_join_df[\"o_orderdate\"] >= right_join_df[\"l_shipdate\"] - pd.Timedelta(days=5)) &\n",
    "    (right_join_df[\"o_orderdate\"] <= right_join_df[\"l_shipdate\"] + pd.Timedelta(days=5))\n",
    "].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0bbefe-3d5b-4475-b515-d683a7fc54b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full join\n",
    "full_join_df = orders_df.merge(\n",
    "    lineitem_df,\n",
    "    left_on=\"o_orderkey\",\n",
    "    right_on=\"l_orderkey\",\n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "full_join_df[\n",
    "    (full_join_df[\"o_orderdate\"] >= full_join_df[\"l_shipdate\"] - pd.Timedelta(days=5)) &\n",
    "    (full_join_df[\"o_orderdate\"] <= full_join_df[\"l_shipdate\"] + pd.Timedelta(days=5))\n",
    "].head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
