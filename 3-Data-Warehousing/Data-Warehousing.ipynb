{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2731690e-7a71-4575-88ba-199632d0f8f4",
   "metadata": {},
   "source": [
    "# The objective of a data warehouse is to enable analyzing historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57846924-97ff-4a8a-9966-e6187e443dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up (if any existing) tpch db file tpch.db\n",
      "The file tpch.db does not exist.\n",
      "Creating TPCH input data at tpch.db\n",
      "Cleaning up (if any existing) sqlite3 db file example.db\n",
      "The file example.db does not exist.\n",
      "Creating sqlite database file at example.db\n"
     ]
    }
   ],
   "source": [
    "! python ../setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "748b1482-fac3-4521-b4f7-b185b599c5d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "PyArrow >= 4.0.0 must be installed; however, it was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.4/lib/python3.12/site-packages/pyspark/sql/pandas/utils.py:53\u001b[0m, in \u001b[0;36mrequire_minimum_pyarrow_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     have_arrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mduckdb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mload_ext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msql\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m conn \u001b[38;5;241m=\u001b[39m duckdb\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpch.db\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msql\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconn --alias duckdb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.4/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2480\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2478\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2480\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2484\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.4/lib/python3.12/site-packages/IPython/core/magics/extension.py:33\u001b[0m, in \u001b[0;36mExtensionMagics.load_ext\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_str:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UsageError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing module name.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextension_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malready loaded\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m extension is already loaded. To reload it, use:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m module_str)\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.4/lib/python3.12/site-packages/IPython/core/extensions.py:62\u001b[0m, in \u001b[0;36mExtensionManager.load_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load an IPython extension by its module name.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mReturns the string \"already loaded\" if the extension is already loaded,\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m\"no load function\" if the module doesn't have a load_ipython_extension\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mfunction, or None if it succeeded.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_str \u001b[38;5;129;01min\u001b[39;00m BUILTINS_EXTS:\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.4/lib/python3.12/site-packages/IPython/core/extensions.py:77\u001b[0m, in \u001b[0;36mExtensionManager._load_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_str \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[0;32m---> 77\u001b[0m         mod \u001b[38;5;241m=\u001b[39m \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     mod \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[module_str]\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_load_ipython_extension(mod):\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.4/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.4/lib/python3.12/site-packages/sql/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmagic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_ipython_extension\n\u001b[1;32m      4\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.10.12\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_ipython_extension\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.4/lib/python3.12/site-packages/sql/magic.py:33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshlex\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_statements\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.4/lib/python3.12/site-packages/sql/connection/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     ConnectionManager,\n\u001b[1;32m      3\u001b[0m     SQLAlchemyConnection,\n\u001b[1;32m      4\u001b[0m     DBAPIConnection,\n\u001b[1;32m      5\u001b[0m     SparkConnectConnection,\n\u001b[1;32m      6\u001b[0m     is_pep249_compliant,\n\u001b[1;32m      7\u001b[0m     is_spark,\n\u001b[1;32m      8\u001b[0m     PLOOMBER_DOCS_LINK_STR,\n\u001b[1;32m      9\u001b[0m     default_alias_for_engine,\n\u001b[1;32m     10\u001b[0m     ResultSetCollection,\n\u001b[1;32m     11\u001b[0m     detect_duckdb_summarize_or_select,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     15\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnectionManager\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSQLAlchemyConnection\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetect_duckdb_summarize_or_select\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m ]\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.4/lib/python3.12/site-packages/sql/connection/connection.py:20\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Engine\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     NoSuchModuleError,\n\u001b[1;32m     13\u001b[0m     OperationalError,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     ProgrammingError,\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparkdataframe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m handle_spark_dataframe\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merror\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UsageError\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msqlglot\u001b[39;00m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.4/lib/python3.12/site-packages/sql/run/sparkdataframe.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrame\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrame \u001b[38;5;28;01mas\u001b[39;00m CDataFrame\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     DataFrame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.4/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SessionNotSameException\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_dependencies\n\u001b[0;32m---> 20\u001b[0m \u001b[43mcheck_dependencies\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     Any,\n\u001b[1;32m     24\u001b[0m     Dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     Type,\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.4/lib/python3.12/site-packages/pyspark/sql/connect/utils.py:34\u001b[0m, in \u001b[0;36mcheck_dependencies\u001b[0;34m(mod_name)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     require_minimum_pandas_version()\n\u001b[0;32m---> 34\u001b[0m     \u001b[43mrequire_minimum_pyarrow_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     require_minimum_grpc_version()\n",
      "File \u001b[0;32m~/.asdf/installs/python/3.12.4/lib/python3.12/site-packages/pyspark/sql/pandas/utils.py:60\u001b[0m, in \u001b[0;36mrequire_minimum_pyarrow_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     raised_error \u001b[38;5;241m=\u001b[39m error\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m have_arrow:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyArrow >= \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must be installed; however, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit was not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m minimum_pyarrow_version\n\u001b[1;32m     63\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mraised_error\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LooseVersion(pyarrow\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m LooseVersion(minimum_pyarrow_version):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyArrow >= \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must be installed; however, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour version was \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (minimum_pyarrow_version, pyarrow\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m     68\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: PyArrow >= 4.0.0 must be installed; however, it was not found."
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext sql\n",
    "conn = duckdb.connect(\"tpch.db\")\n",
    "%sql conn --alias duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf36968c-dd61-4b75-9409-b12ad8d7778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "show tables;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4967f99-71f1-4b49-88bb-7717a1fad4f2",
   "metadata": {},
   "source": [
    "## Understanding how your business works is critical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc91cec-9124-4219-90c8-ce5c4493391b",
   "metadata": {},
   "source": [
    "- data flow from product/upstream team\n",
    "- create a conceptual data flow diagram\n",
    "- Use tpch as example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2194a7ef-20e3-4af3-b5aa-22529f15923b",
   "metadata": {},
   "source": [
    "## Data modeling refers to how your data is stored for historical analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3fa706-6fae-47a1-97b7-82fc890fe6a2",
   "metadata": {},
   "source": [
    "\n",
    "The term analytical querying usually refers to aggregating numerical (spend, count, sum, avg) data from the fact table for specific dimension attribute(s) (e.g., name, nation, date, month) from the dimension tables. Some examples of analytical queries are \n",
    "\n",
    "1. Who are the top 10 suppliers (by totalprice) in the past year?\n",
    "\n",
    "2. What are the average sales per nation per year?\n",
    "\n",
    "3. How do customer market segments perform (sales) month-over-month?\n",
    "\n",
    "The questions above ask about **historically aggregating data from the fact tables for one or more business entities(dimensions)**. \n",
    "Consider the example analytical question below and notice the facts and dimensions.\n",
    "\n",
    "![Analytical query](./images/im_2.png)\n",
    "add: images\n",
    "\n",
    "When we dissect the above analytical query, we see that it involves:\n",
    "\n",
    "1. Joining the fact data with dimension table(s) to get the dimension attributes such as name, region, & brand. In our example, we join the orders fact table with the customer dimension table.\n",
    "\n",
    "2. **Modifying granularity** (aka rollup, Group by) of the joined table to the dimension(s) in question. In our example, this refers to `GROUP BY custkey, YEAR(orderdate).`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c02712-5c18-49b8-bef3-a4130836f6eb",
   "metadata": {},
   "source": [
    "### Kimball dimensional modeling is by far the most popular among all options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce1e077-c1cc-4476-b339-de2fbe09fcc5",
   "metadata": {},
   "source": [
    "### Real life events are called facts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffca1e9b-fed7-44ba-abee-79a453ac17c6",
   "metadata": {},
   "source": [
    " **Facts**: Each row in a fact table represents a business process that occurred. E.g., In our data warehouse, each row in the `orders` fact table will represent an individual order, and each row in the `lineitem` fact table will represent an item sold as part of an order. Each fact row will have a unique identifier; in our case, it's `orderkey` for orders and a combination of `orderkey & linenumber` for lineitem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2813bf7-0ba1-471f-9851-eca328666791",
   "metadata": {},
   "source": [
    "A fact table's **grain (aka granularity, level)** refers to what a row in a fact table represents. For example, in our checkout process, we can have two fact tables, one for the order and another for the individual items in the order. The items table will have one row per item purchased, whereas the order table will have one row per order made.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5406ab1-e6df-4a4b-89f4-5d6017648f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "-- calculating the totalprice of an order (with orderkey = 1) from it's individual items\n",
    "SELECT\n",
    "    orderkey,\n",
    "    round( sum(extendedprice * (1 - discount) * (1 + tax)),\n",
    "        2\n",
    "    ) AS totalprice\n",
    "    -- Formula to calculate price paid after discount & tax\n",
    "FROM\n",
    "    lineitem\n",
    "WHERE\n",
    "    orderkey = 1\n",
    "GROUP BY\n",
    "    orderkey;\n",
    "\n",
    "/*\n",
    " orderkey | totalprice\n",
    "----------+------------\n",
    "        1 |  172799.56\n",
    "*/\n",
    "\n",
    "-- The totalprice of an order (with orderkey = 1)\n",
    "SELECT\n",
    "    orderkey,\n",
    "    totalprice\n",
    "FROM\n",
    "    orders\n",
    "WHERE\n",
    "    orderkey = 1;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b28366-6e54-43fa-9736-f6d7235bf698",
   "metadata": {},
   "source": [
    "### Dimension = Someone/something that interacts with your business"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a05fa-e810-4fdb-bd58-559f436d2741",
   "metadata": {},
   "source": [
    " **Dimension**: Each row in a dimension table represents a business entity that is important to the business. For example, A car parts seller's data warehouse will have a `customer` dimension table, where each row will represent an individual customer. Other examples of dimension tables in a car parts seller's data warehouse would be `supplier` & `part` tables. Techniques such as [SCD2](https://www.startdataengineering.com/post/how-to-join-fact-scd2-tables/#what-is-an-scd2-table-and-why-use-it) are used to store data whose values can change over time (e.g., customers address).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b08643-ab45-4a41-b306-1dc22c8a4fd7",
   "metadata": {},
   "source": [
    "## Most tech companies follow the 3-hop architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c05353e-e2a6-4766-81db-144b2d0bc2cc",
   "metadata": {},
   "source": [
    "Most data teams have their version of the 3-hop architecture. For example, dbt has its own version (stage, intermediate, mart), and Spark has medallion (bronze, silver, gold) architecture.\n",
    "\n",
    "You may be wondering why we need this data flow architecture when we have the results easily with a simple query shown here.\n",
    "\n",
    "While this is a simple example, in most real-world projects you want to have a standard, cleaned and modelled dataset(bronze) that can be use to create specialized dataset for end-users(gold). See below for how our data will flow:\n",
    "                                                                                                                                                                         add: data 3-hop arch image                              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3341a6a-eee1-49c4-b2d4-e2e7ef4b9a22",
   "metadata": {},
   "source": [
    "### Bronze: Extract raw data and confine it to standard names and data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88a7ddb-1891-4ea8-82f4-23fb26207c02",
   "metadata": {},
   "source": [
    "Since our dataset has data from customer, nation, region, order, and lineitem input datasets, we will bring those data into bronze tables. We will keep their names the same as the input datasets.\n",
    "\n",
    "Let's explore the input datasets and create our bronze datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f6506f-e10e-4a70-b0d6-13bea8e45666",
   "metadata": {},
   "source": [
    "### Silver: Model data for analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e20fff-5963-424c-8aad-0917e08f9b21",
   "metadata": {},
   "source": [
    "In the silver layer, the datasets are modeled using one of the popular styles (e.g., Kimball, Data Vault, etc.). We will use Kimball's dimensional model, as it is the most commonly used one and can account for many use cases.\n",
    "\n",
    "**Data modeling**\n",
    "\n",
    "We will create the following datasets\n",
    "\n",
    "1. **dim_customer**: A customer level table with all the necessary attributes of a customer. We will join nation and region data to the cleaned_customer_df to get all the attributes associated with a customer.\n",
    "2. **fct_orders**: An order level fact(an event that happened) table. This will be the same as cleaned_orders_df since the orders table has all the necessary details about the order and how it associates with dimension tables like customer_key.\n",
    "3. **fct_lineitem**: A lineitem (items that are part of an order) fact table. This table will be the same as cleaned_lineitem_df since the lineitem table has all the lineitem level details and keys to associate with dimension tables like partkey and suppkey.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66222144-7915-462c-9596-1cf800437e7f",
   "metadata": {},
   "source": [
    "### Gold: Create tables for end-users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d7fa08-7b1e-47dd-aae6-2e52dc1111e3",
   "metadata": {},
   "source": [
    "The gold layer contains datasets required for the end user. The user-required datasets are fact tables joined with dimension tables aggregated to the necessary grain. In real-world projects, multiple teams/users ask for datasets with differing grains from the same underlying fact and dimension tables. While you can join the necessary tables and aggregate them individually for each ask, it leads to repeated code and joins.\n",
    "\n",
    "To avoid this issue, companies typically do the following:\n",
    "\n",
    "1. **OBT**: This is a fact table with multiple dimension tables left joined with it.\n",
    "2. **pre-aggregated table**: The OBT table rolled up to the end user/team requested grain. The pre-aggregated dataset will be the dataset that the end user accesses. By providing the end user with the exact columns they need, we can ensure that all the metrics are in one place and issues due to incorrect metric calculations by end users are significantly reduced. These tables act as our end-users SOT (source of truth).\n",
    "\n",
    "#### OBT: Join the fact table with all its dimensions\n",
    "\n",
    "In our example, we have two fact tables, fct_orders and fct_lineitem. Since we only have one dimension, dim_customer, we can join fct_orders and dim_customer to create wide_orders. For our use case, we can keep fct_lineitem as wide_lineitem.\n",
    "\n",
    "That said, we can easily see a case where we might need to join parts and supplier data with fct_lineitem to get wide_lineitem. But since our use case doesn't require this, we can skip it!\n",
    "\n",
    "Let's create our OBT tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310cfa47-6e07-49f2-839f-3540052ec7db",
   "metadata": {},
   "source": [
    "#### Pre-aggregated tables: Aggregate OBTs to stakeholder-specific grain\n",
    "\n",
    "According to our data requirements, we need data from customer, orders, and lineitem. Since we already have customer and order data in wide_orders, we can join that with wide_lineitem to get the necessary data.\n",
    "\n",
    "We can call the final dataset customer_outreach_metrics (read this article that discusses the importance of naming).\n",
    "\n",
    "Let's create our final dataset in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fea3821-fb35-4e4a-aa95-54d4eaf75883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
