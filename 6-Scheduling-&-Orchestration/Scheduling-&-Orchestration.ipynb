{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cecd71f-9f02-4c03-9405-59532ecf121f",
   "metadata": {},
   "source": [
    "# Scheduler = service to start pipeline at specified times\n",
    "\n",
    "A scheduler is a system that starts a process (in our case a data pipeline) as specified times or frequency\n",
    "\n",
    "A scheduler system is usually constantly running (e.g. Airflow scheduler, cron, etc) and will check the timetable information (metadata in Airflow, corntab file forcron, etc) periodically to figure out which (if any process) to start.\n",
    "\n",
    "With data pipelines you may need to know when a pipeline was supposed to run, for example if your pipeline is supposed to run at 12:00AM every morning, but gets delayed due to infrastructure scale/unavailability you would still get access to the execution time (12:00AM).\n",
    "\n",
    "Keeping track of exectution time in a pipeline is critical if you want your pipelines to work on a specified set of data per run\n",
    "\n",
    "add: execution time image from max's blog\n",
    "\n",
    "The execution time indicates the input data for most data pipelines and depending on how you desing your pipeline this will play a crucial role. Often times this is used as an unique identifier for a specific run of the pipeline (aka `run_id`).\n",
    "Having a unique identifier per pipeline run( `run_id`) will help you design idempotent, easy to debug pipeline.                 \n",
    "                                                                                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1720a92-d783-40a7-b031-4b5845a780e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add: airflow macros\n",
    "# https://airflow.apache.org/docs/apache-airflow/1.10.3/macros.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccc3162-a380-4a3a-857f-7672fc8ec951",
   "metadata": {},
   "source": [
    "While we saw the available macros in Airflow, most schedulers have similar options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdad09a-9eb0-4060-96c9-f872c4530d06",
   "metadata": {},
   "source": [
    "# Orchestrator = System to ensure tasks in a data pipeline are run in the correct order \n",
    "\n",
    "Orchestrator is system that is reponsible for orderding the tasks in a data pipeline. With a scheduler your pipelines will always run the taks in a specifed order.\n",
    "\n",
    "An orchestrator is responsible for ensuring that your data pipeline is a DAG (directed acyclic graph), i.e that there are no infinte loops in your data pipeline.\n",
    "\n",
    "An orchestrator is often times a python library (e.g. Airflow, Dagster) with features that enable you to order your tasks as you see fit, or systems that have auto generate order of tasks from the code (e.g. dbt uses the `ref` feature to identify data task lineage)\n",
    "\n",
    "Modern orchestrators offer features that are hard to replicate in native Python without extensive code:\n",
    "* branching\n",
    "* dynamic task creation\n",
    "* grouping related tasks \n",
    "* conditional workflow (task a or B or c,,.. -> pass/fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8116949-2a0b-421a-888e-1364003a40b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8530b61a-115f-4fc7-b9ac-a0991efaaee0",
   "metadata": {},
   "source": [
    "**NOTE** ften times shedulers and orchestrators are considered as one system, this is not true. You want to understand where the scheduler ends and orchestrator begins to ensure that you use the right system for the use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3dd477-46ef-4563-951e-7d0fe592ffbf",
   "metadata": {},
   "source": [
    "# Airflow is both a scheduler and an orchestrator\n",
    "\n",
    "Let's take a quick look at Airflow's architecture:\n",
    "\n",
    "add: image from https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/overview.html#basic-airflow-deployment\n",
    "\n",
    "When we start docker we can see Airflow webserver (responsible for the UI) and Airflow scheduler (responsbile for starting the DAGs at the right time)\n",
    "\n",
    "In addtion, we have installed the `apache-airflow-client` which is used for defining data pipelines in our code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4907d0-2c47-4006-b805-784632a9eeeb",
   "metadata": {},
   "source": [
    "# Define data pipeline as a DAG\n",
    "\n",
    "Most orchestrators have their own *way* to define a data pipeline (aka DAG). We can define a DAG in Airflow as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde10706-d312-48a8-bb53-3e4ebc45a8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37f168cb-0c8b-4456-9e74-a6dc928b688b",
   "metadata": {},
   "source": [
    "Note that this is stored within the `./dag` folder which is synced to our containers (via volume mount). In the above DAG, we do the follwing\n",
    "\n",
    "1. run `dbt run`\n",
    "2. Run data quality checks with `dbt test`\n",
    "3. Move the metrics data to `sqlite3` which we will use in the next chapter for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d1b2f0-b7d9-4d1b-981c-a69f8a6c4670",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! docker compose down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01843f37-cfc7-4803-b6f6-ccb46b85e497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n"
     ]
    }
   ],
   "source": [
    "! docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "917336a9-4b79-4210-883e-85a27e47ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this in a terminal in this directory\n",
    "# sudo mkdir -p logs plugins temp dags tests data visualization && sudo chmod -R u=rwx,g=rwx,o=rwx logs plugins temp dags tests data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "058454f0-7971-4b74-95f2-2a721e66494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! docker compose up --build -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30780e1d-193d-4fc1-9397-e5687bda4c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sleep 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "733e409b-f9c4-4a83-8281-8606c0a385a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: ./dags/tpch_warehouse/models/*/.ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "! rm -rf ./dags/tpch_warehouse/models/*/.ipynb_checkpoints # always run before dbt run, caused by notebooks, no need to do this if performed via terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99c764a5-224f-4f1d-aefa-c1ba6c25fafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                                           COMMAND                  CREATED          STATUS                    PORTS                                       NAMES\n",
      "96e73ce0515e   metabase/metabase                               \"/app/run_metabase.sh\"   33 seconds ago   Up 31 seconds             0.0.0.0:3000->3000/tcp, :::3000->3000/tcp   dashboard\n",
      "dd6ea004cf36   6-scheduling--orchestration-airflow-webserver   \"/usr/bin/dumb-init …\"   52 minutes ago   Up 51 minutes (healthy)   0.0.0.0:8080->8080/tcp, :::8080->8080/tcp   webserver\n",
      "3b5d361b77a5   6-scheduling--orchestration-airflow-scheduler   \"/usr/bin/dumb-init …\"   52 minutes ago   Up 52 minutes (healthy)   8080/tcp                                    scheduler\n",
      "827ce29a3da1   postgres:16                                     \"docker-entrypoint.s…\"   52 minutes ago   Up 52 minutes (healthy)   0.0.0.0:5432->5432/tcp, :::5432->5432/tcp   postgres\n"
     ]
    }
   ],
   "source": [
    "! docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7df815d7-fab6-4ce4-98a3-c03230a3be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! docker compose down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb6691-81c2-4d2c-b0c4-2ce84e8d1fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
